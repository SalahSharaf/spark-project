{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Machine Learning with DataFrames and Spark SQL \n","\n","* This notebook is run on an Azure HDInsight Spark 3.5 cluster, with all of the data files loaded into the Blob storage\n","* The notebook kernel used is PySpark3\n","\n","#### Load Data Using an Explicit Schema\n","\n","* This notebook uses data that records details of flights.\n","* First few steps involve exploring the data after loading it into a programmatic data object - the DataFrame. \n","* If the structure of the data is known ahead of time, you can explicitly specify the schema for the DataFrame.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import findspark\n","findspark.init()\n","import pyspark"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["conf = pyspark.SparkConf()\n","conf.set('spark.app.name', 'hello_spark') # Optional configurations\n","conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n","conf.set(\"spark.executor.memory\", \"2g\")\n","conf.set(\"spark.default.parallelism\", \"32\")\n","conf.set(\"spark.akka.frameSize\",\"1000\")\n","sc = pyspark.SparkContext.getOrCreate(conf=conf)\n","spark = pyspark.sql.SQLContext(sc)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+---------+-------+---------------+-------------+--------+--------+\n","|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n","+----------+---------+-------+---------------+-------------+--------+--------+\n","|      2014|       12|      8|            658|           -7|     935|      -5|\n","|      2014|        1|     22|           1040|            5|    1505|       5|\n","|      2014|        3|      9|           1443|           -2|    1652|       2|\n","|      2014|        4|      9|           1705|           45|    1839|      34|\n","|      2014|        3|      9|            754|           -1|    1015|       1|\n","|      2014|        1|     15|           1037|            7|    1352|       2|\n","|      2014|        7|      2|            847|           42|    1041|      51|\n","|      2014|        5|     12|           1655|           -5|    1842|     -18|\n","|      2014|        4|     19|           1236|           -4|    1508|      -7|\n","|      2014|       11|     19|           1812|           -3|    2352|      -4|\n","|      2014|       11|      8|           1653|           -2|    1924|      -1|\n","|      2014|        8|      3|           1120|            0|    1415|       2|\n","|      2014|       10|     30|            811|           21|    1038|      29|\n","|      2014|       11|     12|           2346|           -4|     217|     -28|\n","|      2014|       10|     31|           1314|           89|    1544|     111|\n","|      2014|        1|     29|           2009|            3|    2159|       9|\n","|      2014|       12|     17|           2015|           50|    2150|      41|\n","|      2014|        8|     11|           1017|           -3|    1613|      -7|\n","|      2014|        1|     13|           2156|           -9|     607|     -15|\n","|      2014|        6|      5|           1733|          -12|    1945|     -10|\n","+----------+---------+-------+---------------+-------------+--------+--------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.types import *\n","\n","#\n","flightSchema = StructType([\n","  StructField(\"DayofMonth\", IntegerType(), False),\n","  StructField(\"DayOfWeek\", IntegerType(), False),\n","  StructField(\"Carrier\", StringType(), False),\n","  StructField(\"OriginAirportID\", IntegerType(), False),\n","  StructField(\"DestAirportID\", IntegerType(), False),\n","  StructField(\"DepDelay\", IntegerType(), False),\n","  StructField(\"ArrDelay\", IntegerType(), False),\n","])\n","#\n","flights = spark.read.csv('flights_small.csv', schema=flightSchema, header=True)\n","flights.show()"]},{"cell_type":"markdown","metadata":{},"source":["The data is repartitioned to 32, since we have 2 worker nodes with 16 cores each"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false},"outputs":[{"data":{"text/plain":["5"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["flights = flights.repartition(5)\n","flights.rdd.getNumPartitions()"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+--------------------+----------+------------+----+---+---+\n","|faa|                name|       lat|         lon| alt| tz|dst|\n","+---+--------------------+----------+------------+----+---+---+\n","|04G|   Lansdowne Airport|41.1304722| -80.6195833|1044| -5|  A|\n","|06A|Moton Field Munic...|32.4605722| -85.6800278| 264| -5|  A|\n","|06C| Schaumburg Regional|41.9893408| -88.1012428| 801| -6|  A|\n","|06N|     Randall Airport| 41.431912| -74.3915611| 523| -5|  A|\n","|09J|Jekyll Island Air...|31.0744722| -81.4277778|  11| -4|  A|\n","|0A9|Elizabethton Muni...|36.3712222| -82.1734167|1593| -4|  A|\n","|0G6|Williams County A...|41.4673056| -84.5067778| 730| -5|  A|\n","|0G7|Finger Lakes Regi...|42.8835647| -76.7812318| 492| -5|  A|\n","|0P2|Shoestring Aviati...|39.7948244| -76.6471914|1000| -5|  U|\n","|0S9|Jefferson County ...|48.0538086|-122.8106436| 108| -8|  A|\n","+---+--------------------+----------+------------+----+---+---+\n","only showing top 10 rows\n","\n"]}],"source":["airports = spark.read.csv('airports.csv', header=True, inferSchema=True)\n","airports.show(10)"]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- faa: string (nullable = true)\n"," |-- name: string (nullable = true)\n"," |-- lat: double (nullable = true)\n"," |-- lon: double (nullable = true)\n"," |-- alt: integer (nullable = true)\n"," |-- tz: integer (nullable = true)\n"," |-- dst: string (nullable = true)\n","\n"]}],"source":["# Show the inferred schema for the airports dataframe\n","airports.printSchema()"]},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+--------------------+\n","|dst|                name|\n","+---+--------------------+\n","|  A|   Lansdowne Airport|\n","|  A|Moton Field Munic...|\n","|  A| Schaumburg Regional|\n","|  A|     Randall Airport|\n","|  A|Jekyll Island Air...|\n","+---+--------------------+\n","\n"]}],"source":["cities = airports.select(\"dst\", \"name\")\n","cities.limit(5).show()"]},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+-----+\n","|                name|Count|\n","+--------------------+-----+\n","|   Municipal Airport|50000|\n","|        All Airports|30000|\n","|Jefferson County ...|20000|\n","|Marshfield Munici...|20000|\n","|Douglas Municipal...|20000|\n","+--------------------+-----+\n","\n"]}],"source":["from pyspark.sql import functions as F\n","\n","flightsByOrigin = flights.join(airports).groupBy(\"name\").agg(F.count(F.lit(1)).alias(\"Count\")).orderBy(\"Count\", ascending=False)\n","\n","flightsByOrigin.limit(5).show()"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+--------------------+----------------+-----------------+------------------+------------------+\n","|summary|          DayofMonth|       DayOfWeek|          Carrier|          DepDelay|          ArrDelay|\n","+-------+--------------------+----------------+-----------------+------------------+------------------+\n","|  count|               10000|           10000|            10000|              9945|              9925|\n","|   mean|              2014.0|          6.6438|          15.7009|1477.7236802413272|2.2530982367758186|\n","| stddev|7.190546479957713...|3.31916002059621|8.895142019392068| 526.5936522261676|31.074918600451884|\n","|    min|                2014|               1|                1|                 1|               -58|\n","|    max|                2014|              12|                9|              2400|               900|\n","+-------+--------------------+----------------+-----------------+------------------+------------------+\n","\n"]}],"source":["flights.drop('DestAirportID','OriginAirportID',).describe().show()"]},{"cell_type":"markdown","metadata":{},"source":["### Check the number of partitions for the flights dataframe"]},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false},"outputs":[{"data":{"text/plain":["5"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["flights.rdd.getNumPartitions()"]},{"cell_type":"markdown","metadata":{},"source":["### Determine the Presence of Duplicates\n"]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of duplicate rows =  10\n"]}],"source":["total_flights = flights.count()\n","unique_flights = flights.dropDuplicates().count()\n","\n","print(\"Number of duplicate rows = \",total_flights - unique_flights)"]},{"cell_type":"markdown","metadata":{},"source":["### Identify Missing Values using "]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Missing values =  75\n"]}],"source":["unique_flights_withoutNA =  flights.dropDuplicates()\\\n",".dropna(how=\"any\").count()\n","\n","print(\"Missing values = \", total_flights - unique_flights_withoutNA)"]},{"cell_type":"markdown","metadata":{},"source":["### Cleaning the Data"]},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of rows in cleaned data set =  9990 Number of partitions =  100\n"]}],"source":["data = flights.dropDuplicates().fillna(value=0, subset=[\"ArrDelay\", \"DepDelay\"]).repartition(100)\n","\n","# Let's cache this for efficient future use\n","data.cache()\n","\n","print(\"Number of rows in cleaned data set = \", data.count(), \"Number of partitions = \", data.rdd.getNumPartitions())"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+---------+-------+---------------+-------------+--------+--------+\n","|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n","+----------+---------+-------+---------------+-------------+--------+--------+\n","|         0|        0|      0|              0|            0|       0|       0|\n","+----------+---------+-------+---------------+-------------+--------+--------+\n","\n"]}],"source":["from pyspark.sql.functions import isnan, when, count, col\n","data.select([count(when(isnan(c), c)).alias(c) for c in data.columns]).show()"]},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+----------+------------------+------------------+------------------+------------------+------------------+------------------+\n","|summary|DayofMonth|         DayOfWeek|           Carrier|   OriginAirportID|     DestAirportID|          DepDelay|          ArrDelay|\n","+-------+----------+------------------+------------------+------------------+------------------+------------------+------------------+\n","|  count|      9990|              9990|              9990|              9952|              9952|              9990|              9990|\n","|   mean|    2014.0| 6.648848848848849|15.708108108108108|1277.1158561093248| 6.068629421221865|1471.0672672672672|2.2384384384384384|\n","| stddev|       0.0|3.3169441518562466| 8.895590424824604|  524.114295105588|28.808608062751762| 534.6444220259324|30.974178660329546|\n","|    min|      2014|                 1|                 1|                 1|               -19|                 0|               -58|\n","|    max|      2014|                12|                 9|              2400|               886|              2400|               900|\n","+-------+----------+------------------+------------------+------------------+------------------+------------------+------------------+\n","\n"]}],"source":["data.describe().show()"]},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":false},"outputs":[{"data":{"text/plain":["0.0673822128377539"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["data.corr(\"DepDelay\", \"ArrDelay\")"]},{"cell_type":"markdown","metadata":{},"source":["### Using Spark SQL"]},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+--------------+\n","|DayOfWeek|Avg Delay(min)|\n","+---------+--------------+\n","|        1|          0.89|\n","|        2|          3.42|\n","|        3|          0.59|\n","|        4|          1.55|\n","|        5|          0.01|\n","|        6|          3.28|\n","|        7|          3.61|\n","|        8|          4.09|\n","|        9|          1.60|\n","|       10|          0.76|\n","|       11|         -1.34|\n","|       12|          7.59|\n","+---------+--------------+\n","\n"]}],"source":["data.createOrReplaceTempView(\"flightData\")\n","spark.sql(\"\"\" \n","SELECT DayOfWeek, CAST(AVG(ArrDelay) as DECIMAL(6,2)) AS `Avg Delay(min)` \n","FROM flightData \n","GROUP BY DayOfWeek \n","ORDER BY DayOfWeek \n","\"\"\").show()"]},{"cell_type":"markdown","metadata":{},"source":["## Using SparkML \n","\n","#### Preparing the data for machine learning"]},{"cell_type":"code","execution_count":18,"metadata":{"collapsed":false},"outputs":[],"source":["# Import sql functions and ML libraries\n","from pyspark.sql.functions import *\n","\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.feature import VectorAssembler"]},{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- DayofMonth: integer (nullable = true)\n"," |-- DayOfWeek: integer (nullable = true)\n"," |-- Carrier: string (nullable = true)\n"," |-- OriginAirportID: integer (nullable = true)\n"," |-- DestAirportID: integer (nullable = true)\n"," |-- DepDelay: integer (nullable = true)\n"," |-- ArrDelay: integer (nullable = true)\n","\n"]}],"source":["data.printSchema()"]},{"cell_type":"code","execution_count":20,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of partitions =  100\n"]}],"source":["data = data.select(\"DayofMonth\", \"DayOfWeek\", \"Carrier\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\", \\\n","                   ((col(\"ArrDelay\") > 15).cast(\"Int\").alias(\"label\")))\n","\n","print(\"Number of partitions = \" , data.rdd.getNumPartitions())\n"]},{"cell_type":"markdown","metadata":{},"source":["### Split the data into training and test sets\n"]},{"cell_type":"code","execution_count":21,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Training rows count: 6988  Testing rows count: 3002\n","root\n"," |-- DayofMonth: integer (nullable = true)\n"," |-- DayOfWeek: integer (nullable = true)\n"," |-- Carrier: string (nullable = true)\n"," |-- OriginAirportID: integer (nullable = true)\n"," |-- DestAirportID: integer (nullable = true)\n"," |-- DepDelay: integer (nullable = true)\n"," |-- label: integer (nullable = true)\n","\n","None\n"]}],"source":["splits = data.randomSplit([0.7, 0.3])\n","\n","train = splits[0]\n","# rename the target variable in the test set to trueLabel\n","test = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n","\n","train_rows = train.count()\n","test_rows = test.count()\n","print (\"Training rows count:\", train_rows, \" Testing rows count:\", test_rows)\n","print(data.printSchema())"]},{"cell_type":"markdown","metadata":{},"source":["### Prepare the training data for SparkML\n","\n","In this case, we will create a pipeline with seven stages:\n","1. **StringIndexer** estimator that converts string values to indexes for categorical features\n","2. **VectorAssembler** that combines categorical features into a single vector\n","3. **VectorIndexer** that creates indexes for a vector of categorical features\n","4. **VectorAssembler** that creates a vector of continuous numeric features\n","5. **MinMaxScaler** that normalizes continuous numeric features\n","6. **VectorAssembler** that creates a vector of categorical and continuous features\n","7. **LogisticRegression** classifier that trains a classification model."]},{"cell_type":"code","execution_count":29,"metadata":{"collapsed":false},"outputs":[],"source":["from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer\\\n",", MinMaxScaler\n","from pyspark.ml import Pipeline\n","\n","#Stage 1. convert string values to indexes for categorical features\n","strIdx = StringIndexer(handleInvalid = \"skip\",inputCol = \"Carrier\", outputCol = \"CarrierIdx\")\n","\n","#Stage 2. combine categorical features into a single vector\n","catVect = VectorAssembler(handleInvalid = \"skip\",inputCols = [\"CarrierIdx\", \"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\"], outputCol=\"catFeatures\")\n","\n","#Stage 3. create indexes for a vector of categorical features\n","catIdx = VectorIndexer(handleInvalid = \"skip\",inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\n","\n","#Stage 4. create a vector of continuous numeric features\n","numVect = VectorAssembler(handleInvalid = \"skip\",inputCols = [\"DepDelay\"], outputCol=\"numFeatures\")\n","\n","#Stage 5. normalize continuous numeric features\n","minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n","\n","#Stage 6. creates a vector of categorical and continuous features\n","featVect = VectorAssembler(handleInvalid = \"skip\",inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")\n","\n","#Stage 7. LogisticRegression classifier that trains a classification model\n","lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n","\n","# Now define the pipeline\n","pipeline = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, lr])"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["strIdx = StringIndexer(handleInvalid = \"keep\",inputCol = \"Carrier\", outputCol = \"CarrierIdx\")"]},{"cell_type":"markdown","metadata":{},"source":["### Train the Classification Model\n"]},{"cell_type":"code","execution_count":30,"metadata":{"collapsed":false},"outputs":[{"name":"stderr","output_type":"stream","text":["Exception ignored in: <function JavaWrapper.__del__ at 0x000001DAD411A670>\n","Traceback (most recent call last):\n","  File \"C:\\Spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\", line 42, in __del__\n","    if SparkContext._active_spark_context and self._java_obj is not None:\n","AttributeError: 'MinMaxScaler' object has no attribute '_java_obj'\n"]},{"name":"stdout","output_type":"stream","text":["Model training complete in:  20.038016999999968  seconds\n"]}],"source":["import timeit\n","start_time = timeit.default_timer()\n","\n","piplineModel = pipeline.fit(train)\n","\n","elapsed = timeit.default_timer() - start_time\n","\n","print (\"Model training complete in: \", elapsed, \" seconds\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Test the Pipeline Model"]},{"cell_type":"code","execution_count":31,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------------------------------+----------+---------+\n","|features                                     |prediction|trueLabel|\n","+---------------------------------------------+----------+---------+\n","|[29.0,0.0,0.0,823.0,-7.0,0.5654166666666667] |0.0       |0        |\n","|[5.0,0.0,0.0,1132.0,-8.0,0.5833333333333334] |0.0       |0        |\n","|[9.0,0.0,1.0,837.0,-3.0,0.6875]              |0.0       |0        |\n","|[25.0,0.0,1.0,1327.0,7.0,0.8562500000000001] |0.0       |0        |\n","|[28.0,0.0,1.0,1158.0,19.0,0.6395833333333334]|0.0       |1        |\n","|[24.0,0.0,2.0,1608.0,48.0,0.8079166666666667]|0.0       |1        |\n","|[5.0,0.0,4.0,1302.0,-3.0,0.85375]            |0.0       |0        |\n","|[0.0,0.0,4.0,754.0,-6.0,0.42750000000000005] |0.0       |0        |\n","|[21.0,0.0,4.0,1553.0,23.0,0.9758333333333334]|0.0       |0        |\n","|[11.0,0.0,4.0,1657.0,22.0,0.7629166666666667]|0.0       |1        |\n","+---------------------------------------------+----------+---------+\n","only showing top 10 rows\n","\n"]}],"source":["prediction = piplineModel.transform(test)\n","predicted = prediction.select(\"features\", \"prediction\", \"trueLabel\")\n","predicted.show(10, truncate=False)"]},{"cell_type":"markdown","metadata":{},"source":["Looking at the result, the prediction column contains the predicted value for the label, and the trueLabel column contains the actual known value from the testing data. It looks like there are a mix of correct and incorrect predictions."]},{"cell_type":"markdown","metadata":{},"source":["#### Evaluating the classifier: Compute Confusion Matrix Metrics\n","\n","Classifiers are typically evaluated by creating a confusion matrix, which indicates the number of:\n","\n","True Positives\n","\n","True Negatives\n","\n","False Positives\n","\n","False Negatives\n","\n","From these core measures, other evaluation metrics such as precision and recall can be calculated."]},{"cell_type":"code","execution_count":32,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+-------------------+\n","|   metric|              value|\n","+---------+-------------------+\n","|       TP|               30.0|\n","|       FP|                0.0|\n","|       TN|             2563.0|\n","|       FN|              400.0|\n","|Precision|                1.0|\n","|   Recall|0.06976744186046512|\n","+---------+-------------------+\n","\n"]}],"source":["tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n","fp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n","tn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n","fn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n","metrics = spark.createDataFrame([\n"," (\"TP\", tp),\n"," (\"FP\", fp),\n"," (\"TN\", tn),\n"," (\"FN\", fn),\n"," (\"Precision\", tp / (tp + fp)),\n"," (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\n","metrics.show()"]},{"cell_type":"markdown","metadata":{"collapsed":true},"source":["#### View the Raw Prediction and Probability\n"]},{"cell_type":"code","execution_count":33,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------------+----------------------------------------+----------+---------+\n","|rawPrediction                           |probability                             |prediction|trueLabel|\n","+----------------------------------------+----------------------------------------+----------+---------+\n","|[2.0534829460703086,-2.0534829460703086]|[0.8862990782407317,0.11370092175926823]|0.0       |0        |\n","|[2.0161185487425604,-2.0161185487425604]|[0.8824790619429103,0.11752093805708964]|0.0       |0        |\n","|[1.94381483835791,-1.94381483835791]    |[0.8747706452628454,0.1252293547371545] |0.0       |0        |\n","|[1.6971219918356182,-1.6971219918356182]|[0.8451584765993352,0.15484152340066482]|0.0       |0        |\n","|[1.5712993996187028,-1.5712993996187028]|[0.8279687690649281,0.1720312309350719] |0.0       |1        |\n","|[1.0045882650310214,-1.0045882650310214]|[0.7319597293449105,0.2680402706550895] |0.0       |1        |\n","|[1.8381237631571539,-1.8381237631571539]|[0.8627266570570166,0.13727334294298346]|0.0       |0        |\n","|[2.041153520939851,-2.041153520939851]  |[0.8850506746701555,0.11494932532984455]|0.0       |0        |\n","|[1.3693683268753474,-1.3693683268753474]|[0.7972780781013438,0.20272192189865634]|0.0       |0        |\n","|[1.4148188675969084,-1.4148188675969084]|[0.8045248931317956,0.19547510686820438]|0.0       |1        |\n","+----------------------------------------+----------------------------------------+----------+---------+\n","only showing top 10 rows\n","\n"]}],"source":["prediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\")\\\n",".show(10, truncate=False)"]},{"cell_type":"markdown","metadata":{},"source":["#### Review the Area Under ROC"]},{"cell_type":"code","execution_count":34,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Area under the ROC curve =  0.8910320391256612\n"]}],"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","evaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n","aur = evaluator.evaluate(prediction)\n","print (\"Area under the ROC curve = \", aur)"]},{"cell_type":"markdown","metadata":{},"source":["#### Change the Discrimination Threshold\n","\n","The AUC score seems to indicate a reasonably good model, but the performance metrics seem to indicate that it predicts a high number of False Negative labels (i.e. it predicts 0 when the true label is 1), leading to a low Recall. \n","\n","You can affect the way a model performs by changing its parameters. For example, as noted previously, the default discrimination threshold is set to 0.5 - so if there are a lot of False Positives, you may want to consider raising this; or conversely, you may want to address a large number of False Negatives by lowering the threshold."]},{"cell_type":"code","execution_count":35,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------------+----------------------------------------+----------+---------+\n","|rawPrediction                           |probability                             |prediction|trueLabel|\n","+----------------------------------------+----------------------------------------+----------+---------+\n","|[2.053482946070309,-2.053482946070309]  |[0.8862990782407317,0.11370092175926819]|0.0       |0        |\n","|[2.016118548742561,-2.016118548742561]  |[0.8824790619429105,0.1175209380570896] |0.0       |0        |\n","|[1.94381483835791,-1.94381483835791]    |[0.8747706452628454,0.1252293547371545] |0.0       |0        |\n","|[1.6971219918356184,-1.6971219918356184]|[0.8451584765993352,0.1548415234006648] |0.0       |0        |\n","|[1.5712993996187028,-1.5712993996187028]|[0.8279687690649281,0.1720312309350719] |0.0       |1        |\n","|[1.004588265031021,-1.004588265031021]  |[0.7319597293449104,0.2680402706550896] |0.0       |1        |\n","|[1.838123763157154,-1.838123763157154]  |[0.8627266570570166,0.13727334294298343]|0.0       |0        |\n","|[2.041153520939851,-2.041153520939851]  |[0.8850506746701555,0.11494932532984455]|0.0       |0        |\n","|[1.3693683268753474,-1.3693683268753474]|[0.7972780781013438,0.20272192189865634]|0.0       |0        |\n","|[1.4148188675969082,-1.4148188675969082]|[0.8045248931317956,0.1954751068682044] |0.0       |1        |\n","+----------------------------------------+----------------------------------------+----------+---------+\n","only showing top 10 rows\n","\n"]}],"source":["#Change the threshold to 0.3 and create a new LogisticRegression model\n","lr2 = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter=10,regParam=0.3, threshold=0.35)\n","\n","#Set up new pipeline\n","pipeline2 = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, lr2])\n","model2 = pipeline2.fit(train)\n","\n","#Make new predictions\n","newPrediction = model2.transform(test)\n","newPrediction.select(\"rawPrediction\", \"probability\", \"prediction\", \"trueLabel\")\\\n",".show(10, truncate=False)"]},{"cell_type":"code","execution_count":37,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+-------------------+\n","|   metric|              value|\n","+---------+-------------------+\n","|       TP|               71.0|\n","|       FP|                0.0|\n","|       TN|             2563.0|\n","|       FN|              359.0|\n","|Precision|                1.0|\n","|   Recall|0.16511627906976745|\n","+---------+-------------------+\n","\n"]}],"source":["# Recalculate confusion matrix, using the new predictions\n","tp2 = float(newPrediction.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n","fp2 = float(newPrediction.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n","tn2 = float(newPrediction.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n","fn2 = float(newPrediction.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n","\n","metrics2 = spark.createDataFrame([\n"," (\"TP\", tp2),\n"," (\"FP\", fp2),\n"," (\"TN\", tn2),\n"," (\"FN\", fn2),\n"," (\"Precision\", tp2 / (tp2 + fp2)),\n"," (\"Recall\", tp2 / (tp2 + fn2))],[\"metric\", \"value\"])\n","metrics2.show()"]},{"cell_type":"markdown","metadata":{},"source":["Note that there are now more True Positives and less False Negatives, and Recall has improved. By changing the discrimination threshold, the model now gets more predictions correct - though it's worth noting that the number of False Positives has also increased."]},{"cell_type":"markdown","metadata":{},"source":["#### Tune Parameters using cross validation with grid search\n","\n","You can tune parameters to find the best model for your data. To do this we can use the **CrossValidator** class to evaluate each combination of parameters defined in a **ParameterGrid** against multiple folds of the data split into training and validation datasets, in order to find the best performing parameters. \n","\n","* Note that this can take a long time to run because every parameter combination is tried multiple times."]},{"cell_type":"code","execution_count":38,"metadata":{"collapsed":false},"outputs":[],"source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","paramGrid = ParamGridBuilder()\\\n",".addGrid(lr.regParam, [0.3])\\\n",".addGrid(lr.maxIter, [10])\\\n",".addGrid(lr.threshold, [0.25, 0.3, 0.35])\\\n",".build()\n","\n","cv = CrossValidator(estimator=pipeline, evaluator=BinaryClassificationEvaluator(),\\\n","                    estimatorParamMaps=paramGrid, numFolds=5)\n","\n","modelCV = cv.fit(train)"]},{"cell_type":"markdown","metadata":{},"source":["#### Test the Model\n","\n","Now we're ready to apply the model to the test data."]},{"cell_type":"code","execution_count":39,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------------------------------+----------+---------+\n","|features                                     |prediction|trueLabel|\n","+---------------------------------------------+----------+---------+\n","|[29.0,0.0,0.0,823.0,-7.0,0.5654166666666667] |0.0       |0        |\n","|[5.0,0.0,0.0,1132.0,-8.0,0.5833333333333334] |0.0       |0        |\n","|[9.0,0.0,1.0,837.0,-3.0,0.6875]              |0.0       |0        |\n","|[25.0,0.0,1.0,1327.0,7.0,0.8562500000000001] |0.0       |0        |\n","|[28.0,0.0,1.0,1158.0,19.0,0.6395833333333334]|0.0       |1        |\n","|[24.0,0.0,2.0,1608.0,48.0,0.8079166666666667]|1.0       |1        |\n","|[5.0,0.0,4.0,1302.0,-3.0,0.85375]            |0.0       |0        |\n","|[0.0,0.0,4.0,754.0,-6.0,0.42750000000000005] |0.0       |0        |\n","|[21.0,0.0,4.0,1553.0,23.0,0.9758333333333334]|0.0       |0        |\n","|[11.0,0.0,4.0,1657.0,22.0,0.7629166666666667]|0.0       |1        |\n","+---------------------------------------------+----------+---------+\n","only showing top 10 rows\n","\n"]}],"source":["predictionCV = modelCV.transform(test)\n","predictedCV = predictionCV.select(\"features\", \"prediction\", \"trueLabel\")\n","predictedCV.show(10, truncate=False)"]},{"cell_type":"code","execution_count":40,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+-------------------+\n","|   metric|              value|\n","+---------+-------------------+\n","|       TP|              165.0|\n","|       FP|                1.0|\n","|       TN|             2562.0|\n","|       FN|              265.0|\n","|Precision| 0.9939759036144579|\n","|   Recall|0.38372093023255816|\n","+---------+-------------------+\n","\n"]}],"source":["# Recalculate confusion matrix, using the new predictions\n","tp3 = float(predictionCV.filter(\"prediction == 1.0 AND truelabel == 1\").count())\n","fp3 = float(predictionCV.filter(\"prediction == 1.0 AND truelabel == 0\").count())\n","tn3 = float(predictionCV.filter(\"prediction == 0.0 AND truelabel == 0\").count())\n","fn3 = float(predictionCV.filter(\"prediction == 0.0 AND truelabel == 1\").count())\n","\n","metrics3 = spark.createDataFrame([\n"," (\"TP\", tp3),\n"," (\"FP\", fp3),\n"," (\"TN\", tn3),\n"," (\"FN\", fn3),\n"," (\"Precision\", tp3 / (tp3 + fp3)),\n"," (\"Recall\", tp3 / (tp3 + fn3))],[\"metric\", \"value\"])\n","metrics3.show()"]},{"cell_type":"markdown","metadata":{},"source":["Note that the recall metrics has improved."]},{"cell_type":"code","execution_count":54,"metadata":{"collapsed":false},"outputs":[],"source":["bestPipeline = modelCV.bestModel\n","bestLRModel = bestPipeline.stages[6]\n","bestParams = bestLRModel.extractParamMap()"]},{"cell_type":"code","execution_count":55,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Key:  LogisticRegression_c6a4bce94b9c__aggregationDepth  ---> Value =  2\n","Key:  LogisticRegression_c6a4bce94b9c__elasticNetParam  ---> Value =  0.0\n","Key:  LogisticRegression_c6a4bce94b9c__family  ---> Value =  auto\n","Key:  LogisticRegression_c6a4bce94b9c__featuresCol  ---> Value =  features\n","Key:  LogisticRegression_c6a4bce94b9c__fitIntercept  ---> Value =  True\n","Key:  LogisticRegression_c6a4bce94b9c__labelCol  ---> Value =  label\n","Key:  LogisticRegression_c6a4bce94b9c__maxIter  ---> Value =  10\n","Key:  LogisticRegression_c6a4bce94b9c__predictionCol  ---> Value =  prediction\n","Key:  LogisticRegression_c6a4bce94b9c__probabilityCol  ---> Value =  probability\n","Key:  LogisticRegression_c6a4bce94b9c__rawPredictionCol  ---> Value =  rawPrediction\n","Key:  LogisticRegression_c6a4bce94b9c__regParam  ---> Value =  0.3\n","Key:  LogisticRegression_c6a4bce94b9c__standardization  ---> Value =  True\n","Key:  LogisticRegression_c6a4bce94b9c__threshold  ---> Value =  0.25\n","Key:  LogisticRegression_c6a4bce94b9c__tol  ---> Value =  1e-06\n"]}],"source":["#type(bestParams)\n","for k,v in bestParams.items():\n","    print(\"Key: \", k, \" ---> Value = \", v)"]},{"cell_type":"markdown","metadata":{},"source":["The improvement in recall was obtained with the change in threshold (Original: 0.35, Tuned: 0.30)"]},{"cell_type":"code","execution_count":56,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Area under the ROC curve =  0.8910188823054379\n"]}],"source":["eval2 = BinaryClassificationEvaluator(labelCol=\"trueLabel\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n","aur2 = eval2.evaluate(predictionCV)\n","print (\"Area under the ROC curve = \", aur2)"]},{"cell_type":"markdown","metadata":{"collapsed":true},"source":["No significant reduction in Area under the ROC curve."]}],"metadata":{"interpreter":{"hash":"6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"},"kernelspec":{"display_name":"PySpark3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":1}
